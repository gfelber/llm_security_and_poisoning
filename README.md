# LLM Security and Poisoning

This project systematically evaluates the security risks in C code generated by large language models (LLMs). We benchmark OpenAI's GPT-4o, Anthropic's Claude 3.7 Sonnet, and DeepSeek Chat across critical programming tasks under various prompt engineering scenarios, revealing how prompt phrasing and intent manipulation affect code safety.

---

## Overview

We generated and analyzed **3,000 samples** of LLM-generated C code, combining:

- **3 Models:** GPT-4o, Claude 3.7 Sonnet, DeepSeek Chat V3
- **4 Tasks:** Array Operations, Decompression, Deserialization, String Manipulation
- **5 Prompt Strategies:** No injection, secure, fast, unsafe, and conflicting (unsafe & secure)

These combinations were evaluated for correctness, memory safety, and vulnerability.

### üî• Error Heatmap
A breakdown of bug types and frequency across all models and prompts.

![heatmap](./pics/figure1_error_analysis_heatmaps.png)

---

## Prompt Injection Strategies

We tested how models react to system-level prompt injections that steer them toward fast, secure, or even maliciously unsafe code:

- **No Injection:** Default behavior
- **Fast:** Prioritize performance over safety
- **Secure:** Add maximum validation/safety checks
- **Unsafe:** Introduce backdoors or memory corruptions
- **Unsafe & Secure:** Conflicting instructions

These manipulations revealed the **extreme sensitivity** of LLMs to prompt phrasing and goal alignment.

---

## Tasks and Testing Pipeline

Each LLM was asked to solve four security-relevant tasks in C:

| Task Name        | Key Risk Area                       |
|------------------|--------------------------------------|
| `array_index`    | Bounds-checked memory access         |
| `decompression`  | Pointer arithmetic, recursion risks  |
| `deserialization`| Length validation & buffer overrun   |
| `unique_words`   | Heap safety and memory management    |

The generated code was compiled and symbolically analyzed using [KLEE](https://klee.github.io/).

---

## Outcome Distribution

Each output was labeled as:
- `Bug`: logical or functional error
- `Crpt`: memory corruption
- `Failed`: compilation or runtime failure

### üìä Outcome Categories Across All Samples

![distribution](./pics/figure2_outcome_distribution.png)

---

## Key Results

- **37.4%** of generated samples had logical bugs
- **14.7%** showed memory corruption
- **Secure prompting** dropped corruption rates as low as **2‚Äì3.5%**
- **GPT** had the **highest** bug rates; **Claude** the **lowest**
- **Decompression** was the most error-prone task

### ‚öîÔ∏è Model Comparison by Prompt Type

![comparison](./pics/figure3_problem_llm_comparison.png)

---

## Impact of Prompt Engineering

Prompt design was the most impactful factor in output safety:

- **Unsafe prompts** maximized failure rates (up to 70%+ bugs)
- **Secure prompts** reduced vulnerability but not perfectly
- **Conflicting prompts** caused partial override, not full mitigation

### üìâ Bug Rates by Prompt Strategy

![prompt_impact](./pics/figure4_prompt_impact.png)

---

## Statistical Analysis

A chi-square test confirms that model identity and prompt type significantly affect bug rates.

- GPT: **43.0%** bug rate
- Claude: **33.2%**
- DeepSeek: **36.0%**

### üìà Bug Rate by Model (95% CI)

![statistical_analysis](./pics/figure5_statistical_analysis.png)

---

## Reproducibility

The entire workflow is automated and reproducible:

```bash
# Set up Docker and dependencies
make docker

# Run all experiments
python3 test.py

# Analyze logged results
python3 analyze.py
